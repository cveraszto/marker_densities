{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nrrd\n",
    "from os.path import *\n",
    "from pylab import *\n",
    "from matplotlib.ticker import FixedLocator\n",
    "from JSONread import *\n",
    "from density_function import *\n",
    "import json\n",
    "\n",
    "DATA_FOLDER = \"data/\"\n",
    "OUTPUT_FOLDER = \"output/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SMALL_SIZE = 16\n",
    "MEDIUM_SIZE = 18\n",
    "BIGGER_SIZE = 24\n",
    "\n",
    "plt.rc('font', size=SMALL_SIZE)          # controls default text sizes\n",
    "plt.rc('axes', titlesize=BIGGER_SIZE)     # fontsize of the axes title\n",
    "plt.rc('axes', labelsize=MEDIUM_SIZE)    # fontsize of the x and y labels\n",
    "plt.rc('xtick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('ytick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('legend', fontsize=SMALL_SIZE)    # legend fontsize\n",
    "plt.rc('figure', titlesize=BIGGER_SIZE)  # fontsize of the figure title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation, h = nrrd.read(join(DATA_FOLDER, \"annotations.nrrd\"))\n",
    "rv = json.loads(open(join(DATA_FOLDER, \"volumes_25.json\"), \"r\").read())\n",
    "voxel_volume = 25**3/1.0e9\n",
    "neu_dens, h = nrrd.read(join(DATA_FOLDER, \"neu_density.nrrd\"))\n",
    "num_neurons = json.loads(open(join(DATA_FOLDER, \"neuron_counts.json\"), \"r\").read())\n",
    "jsontextfile = open(join(DATA_FOLDER, \"brain_regions.json\"), \"r\")\n",
    "jsoncontent = json.loads(jsontextfile.read())\n",
    "search_children(jsoncontent['msg'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dens_GAD, h = nrrd.read(join(OUTPUT_FOLDER, \"densities_GAD.nrrd\"))\n",
    "dens_GAD_std = json.loads(open(join(OUTPUT_FOLDER, \"std_GAD.json\"), \"r\").read())\n",
    "dens_PV, h = nrrd.read(join(OUTPUT_FOLDER, \"densities_PV.nrrd\"))\n",
    "dens_PV_std = json.loads(open(join(OUTPUT_FOLDER, \"std_PV.json\"), \"r\").read())\n",
    "dens_SST, h = nrrd.read(join(OUTPUT_FOLDER, \"densities_SST.nrrd\"))\n",
    "dens_SST_std = json.loads(open(join(OUTPUT_FOLDER, \"std_SST.json\"), \"r\").read())\n",
    "dens_VIP, h = nrrd.read(join(OUTPUT_FOLDER, \"densities_VIP.nrrd\"))\n",
    "dens_VIP_std = json.loads(open(join(OUTPUT_FOLDER, \"std_VIP.json\"), \"r\").read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert cell/mm3 to cell/voxel\n",
    "dens_GAD *= voxel_volume\n",
    "dens_PV *= voxel_volume\n",
    "dens_SST *= voxel_volume\n",
    "dens_VIP *= voxel_volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dens_sum = dens_PV + dens_SST + dens_VIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniques = find_unique_regions(annotation, \n",
    "                        id_to_region_dictionary_ALLNAME, \n",
    "                        region_dictionary_to_id_ALLNAME,\n",
    "                        region_dictionary_to_id_ALLNAME_parent, \n",
    "                        name2allname)\n",
    "\n",
    "children, order_ = find_children(uniques, id_to_region_dictionary_ALLNAME, is_leaf,\n",
    "                                  region_dictionary_to_id_ALLNAME_parent, \n",
    "                                  region_dictionary_to_id_ALLNAME)\n",
    "uniques = uniques[np.argsort(order_)] # order from leaf to biggest regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = []\n",
    "corrected = []\n",
    "num_GADs = []\n",
    "num_SSTs = []\n",
    "num_VIPs = []\n",
    "num_PVs = []\n",
    "low_std_GADs = []\n",
    "up_std_GADs = []\n",
    "low_std_sums = [] \n",
    "up_std_sums = []\n",
    "volumes = []\n",
    "\n",
    "apply_correction = True\n",
    "epsilon = 1e-3 # precision for corrections\n",
    "for id_reg in uniques:\n",
    "    region_name = id_to_region_dictionary_ALLNAME[id_reg]\n",
    "    filter_ = filter_region(annotation, region_name, children, is_leaf, region_dictionary_to_id_ALLNAME)\n",
    "    \n",
    "    num_GAD = np.sum(dens_GAD[filter_])\n",
    "    num_sum = np.sum(dens_sum[filter_]) \n",
    "    if num_sum-num_GAD>1:\n",
    "        low_GAD = dens_GAD_std[str(id_reg)][0]\n",
    "        up_GAD = dens_GAD_std[str(id_reg)][1]\n",
    "        low_sum = dens_PV_std[str(id_reg)][0] + dens_VIP_std[str(id_reg)][0] + dens_SST_std[str(id_reg)][0]\n",
    "        up_sum = min(dens_PV_std[str(id_reg)][1] + dens_VIP_std[str(id_reg)][1] + dens_SST_std[str(id_reg)][1], np.sum(neu_dens[filter_]))\n",
    "        if low_GAD > num_GAD:\n",
    "            if low_GAD - num_GAD>1:\n",
    "                print(\"GAD Error with \" + region_name)\n",
    "                print(\"low: \" + str(low_GAD), \"num: \" + str(num_GAD))\n",
    "            low_GAD = num_GAD\n",
    "        if up_GAD < num_GAD:\n",
    "            if num_GAD - up_GAD>1:\n",
    "                print(\"GAD Error with \" + region_name)\n",
    "                print(\"num: \" + str(num_GAD), \"up: \" + str(up_GAD))\n",
    "            up_GAD = num_GAD\n",
    "        \n",
    "        if low_sum > num_sum:\n",
    "            if low_sum - num_sum>1:\n",
    "                print(\"Sum Error with \" + region_name)\n",
    "                print(\"low: \" + str(low_sum), \"num: \" + str(num_sum))\n",
    "            low_sum = num_sum\n",
    "        if up_sum < num_sum:\n",
    "            if num_sum - up_sum>1:\n",
    "                print(\"Sum Error with \" + region_name)\n",
    "                print(\"num: \" + str(num_sum), \"up: \" + str(up_sum))\n",
    "            up_sum = num_sum\n",
    "        \n",
    "        if apply_correction and low_sum<=up_GAD: # Still correct\n",
    "            corrected.append(True)\n",
    "            # Process ratio of std to correct\n",
    "            ratio  = (num_sum - num_GAD) / (num_sum - num_GAD + up_GAD - low_sum)  \n",
    "\n",
    "            num_GAD = num_GAD + ratio * (up_GAD - num_GAD)\n",
    "            low_GAD = max(low_sum, dens_GAD_std[str(id_reg)][0])\n",
    "\n",
    "            loc_mean_markers = np.array([np.sum(dens_PV[filter_]), np.sum(dens_SST[filter_]), np.sum(dens_VIP[filter_])])\n",
    "            loc_max_markers = np.array([dens_PV_std[str(id_reg)][1], dens_SST_std[str(id_reg)][1], dens_VIP_std[str(id_reg)][1]])\n",
    "            \n",
    "            marker_sums = np.ones(loc_mean_markers.shape) * num_sum\n",
    "            ratios = np.divide(loc_mean_markers, marker_sums, out=np.zeros_like(loc_mean_markers), where=marker_sums!=0)\n",
    "            loc_max_markers = np.minimum(ratios * up_GAD, loc_max_markers)\n",
    "            loc_mean_markers = ratios * (num_GAD - epsilon) # The placing algorithm will place at least num_GAD - epsilon cells  \n",
    "            \n",
    "            loc_filter = np.copy(filter_) \n",
    "            if not is_leaf[region_name]:\n",
    "                # Remaining neurons to place in the whole region\n",
    "                loc_filter = loc_filter * (annotation != id_reg)\n",
    "                num_placed_GAD = np.sum(dens_GAD[loc_filter])\n",
    "                num_placed = np.array([np.sum(dens_PV[loc_filter]), np.sum(dens_SST[loc_filter]), np.sum(dens_VIP[loc_filter])])\n",
    "                \n",
    "                # number of neuron to place in regions where annotation = id_reg\n",
    "                loc_filter = annotation == id_reg\n",
    "                num_GAD = max(num_GAD - num_placed_GAD, 0.)\n",
    "                loc_mean_markers = np.maximum(loc_mean_markers - num_placed, 0.)\n",
    "            else:\n",
    "                num_placed = [0., 0., 0.]\n",
    "                num_placed_GAD = 0.\n",
    "            loc_neu_dens = neu_dens[loc_filter]\n",
    "            sum_dens = np.zeros(annotation.shape)[loc_filter]\n",
    "            \n",
    "            # GAD new density becomes top_density for SST, PV, VIP \n",
    "            loc_neu_dens = fill_density_dataset(num_GAD, np.ones(annotation.shape)[loc_filter], loc_neu_dens, None, epsilon=epsilon)\n",
    "            dens_GAD[loc_filter] = np.copy(loc_neu_dens)\n",
    "            num_GAD = np.sum(loc_neu_dens) + num_placed_GAD\n",
    "            low_GAD = min(low_GAD, num_GAD)\n",
    "            up_GAD = max(up_GAD, num_GAD)\n",
    "\n",
    "            dens_placed = fill_density_dataset(loc_mean_markers[0], np.ones(annotation.shape)[loc_filter], loc_neu_dens - sum_dens, None, epsilon=epsilon)\n",
    "            dens_PV[loc_filter] = np.copy(dens_placed)\n",
    "            num_sum = np.sum(dens_placed)\n",
    "            sum_dens += dens_placed\n",
    "            \n",
    "            dens_placed = fill_density_dataset(loc_mean_markers[1], np.ones(annotation.shape)[loc_filter], loc_neu_dens - sum_dens, None, epsilon=epsilon)\n",
    "            dens_SST[loc_filter] = np.copy(dens_placed)\n",
    "            num_sum += np.sum(dens_placed)\n",
    "            sum_dens += dens_placed\n",
    "\n",
    "            dens_placed = fill_density_dataset(loc_mean_markers[2], np.ones(annotation.shape)[loc_filter], loc_neu_dens - sum_dens, None, epsilon=epsilon)\n",
    "            dens_VIP[loc_filter] = np.copy(dens_placed)\n",
    "            num_sum += np.sum(dens_placed) + np.sum(num_placed)\n",
    "            low_sum = min(low_sum, num_sum)\n",
    "            up_sum = max(np.sum(loc_max_markers), num_sum)\n",
    "            dens_sum = dens_PV + dens_SST + dens_VIP\n",
    "        else:\n",
    "            corrected.append(False)\n",
    "        names.append(region_name)\n",
    "        num_GADs.append(num_GAD)\n",
    "        num_SSTs.append(np.sum(dens_SST[filter_]))\n",
    "        num_PVs.append(np.sum(dens_PV[filter_]))\n",
    "        num_VIPs.append(np.sum(dens_VIP[filter_]))\n",
    "        \n",
    "        low_std_GADs.append(num_GAD - low_GAD)\n",
    "        up_std_GADs.append(up_GAD - num_GAD)\n",
    "        low_std_sums.append(num_sum - low_sum)\n",
    "        up_std_sums.append(up_sum - num_sum)\n",
    "        volumes.append(rv[region_name])\n",
    "filter_ = np.argsort(names)\n",
    "names = np.array(names)[filter_]\n",
    "corrected = np.array(corrected)[filter_]\n",
    "volumes = np.array(volumes)[filter_]\n",
    "num_GADs = np.array(num_GADs)[filter_]\n",
    "num_SSTs = np.array(num_SSTs)[filter_]\n",
    "num_PVs = np.array(num_PVs)[filter_]\n",
    "num_VIPs = np.array(num_VIPs)[filter_]\n",
    "std_GADs = np.vstack((low_std_GADs, up_std_GADs))[:, filter_]\n",
    "std_sums = np.vstack((low_std_sums, up_std_sums))[:, filter_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc_names = np.copy(names)\n",
    "for iname, name in enumerate(names):\n",
    "    loc_names[iname] = allname2name[name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "percent_inh = 19.64 # old percent from Cell Atlas\n",
    "\n",
    "fig, ax = subplots(figsize=(15,2))\n",
    "rect = ax.barh([1], [100.], height=0.5, color = \"blue\", linewidth=1.0, edgecolor=\"black\")\n",
    "ax.annotate(\"{:04.2f}%\".format(100.-percent_inh), xy=(rect[0].get_width() / 2, 1),\n",
    "                    xytext=(0, 0), color='white',\n",
    "                    textcoords=\"offset points\",\n",
    "                    ha='left', va='center',  weight='bold')\n",
    "rect = ax.barh([1], [percent_inh], height=0.5, color = \"red\", linewidth=1.0, edgecolor=\"black\")\n",
    "ax.annotate(\"{:04.2f}%\".format(percent_inh), xy=(rect[0].get_width() / 2, 1),\n",
    "                    xytext=(0, 0), color='white',\n",
    "                    textcoords=\"offset points\",\n",
    "                    ha='center', va='center',  weight='bold')\n",
    "ax.set_xticks(np.linspace(0, 100, 11, endpoint=True))\n",
    "ax.set_xlim([0, 100])\n",
    "ax.get_xaxis().set_tick_params(labelsize=12)\n",
    "ax.set_ylim([0.75, 1.25])\n",
    "ax.get_yaxis().set_visible(False)\n",
    "ax.set_title(\"% of neurons\", fontdict={\"size\":12})\n",
    "tight_layout()\n",
    "savefig(join(OUTPUT_FOLDER,\"old_Percentage_inh_exc.png\"), dpi=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Percentage of GAD / number of neuron\n",
    "percent_inh = np.sum(dens_GAD)/np.sum(neu_dens) *100. \n",
    "# Percentage of PV, VIP, SST / number of neuron \n",
    "percent_sum = np.sum(dens_sum)/np.sum(neu_dens) *100.\n",
    "percent_VIP = np.sum(dens_VIP)/np.sum(neu_dens) *100.\n",
    "percent_PV = np.sum(dens_PV)/np.sum(neu_dens) *100.\n",
    "percent_SST = np.sum(dens_SST)/np.sum(neu_dens) *100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = subplots(figsize=(15,2))\n",
    "rect = ax.barh([1], [100.], height=0.5, color = \"blue\", linewidth=1.0, edgecolor=\"black\")\n",
    "ax.annotate(\"{:04.2f}%\".format(100.-percent_inh), xy=(rect[0].get_width() / 2, 1),\n",
    "                    xytext=(0, 0), color='white',\n",
    "                    textcoords=\"offset points\",\n",
    "                    ha='left', va='center',  weight='bold')\n",
    "rect = ax.barh([1], [percent_inh], height=0.5, color = \"red\", linewidth=1.0, edgecolor=\"black\")\n",
    "ax.annotate(\"{:04.2f}%\".format(percent_inh), xy=(rect[0].get_width() / 2, 1),\n",
    "                    xytext=(0, 0), color='white',\n",
    "                    textcoords=\"offset points\",\n",
    "                    ha='left', va='center',  weight='bold')\n",
    "rectVIP = ax.barh([1], [percent_PV+percent_SST+percent_VIP], height=0.5, color = \"orange\", linewidth=1.0, edgecolor=\"black\")\n",
    "rectSST = ax.barh([1], [percent_PV+percent_SST], height=0.5, color = \"darkblue\", linewidth=1.0, edgecolor=\"black\")\n",
    "rectPV = ax.barh([1], [percent_PV], height=0.5, color = \"green\", linewidth=1.0, edgecolor=\"black\")\n",
    "\n",
    "ax.annotate(\"{:04.2f}%\".format(percent_VIP), \n",
    "                    xy=(rectSST[0].get_width() + (rectVIP[0].get_width() - rectSST[0].get_width()) / 2, 1),\n",
    "                    xytext=(25, 60), color='black',\n",
    "                    textcoords=\"offset points\",\n",
    "                    ha='left', va='center',  weight='bold', arrowprops=dict(arrowstyle=\"-\", color=\"black\",\n",
    "                                shrinkA=10, shrinkB=0,\n",
    "                                patchA=None, patchB=None,\n",
    "                                connectionstyle=\"arc,rad=0\"\n",
    "                                ))\n",
    "ax.annotate(\"{:04.2f}%\".format(percent_SST), \n",
    "                    xy=(rectPV[0].get_width() + (rectSST[0].get_width() - rectPV[0].get_width()) / 2, 1),\n",
    "                    xytext=(0, 60), color='black',\n",
    "                    textcoords=\"offset points\",\n",
    "                    ha='center', va='center',  weight='bold', arrowprops=dict(arrowstyle=\"-\", color=\"black\",\n",
    "                                shrinkA=10, shrinkB=0,\n",
    "                                patchA=None, patchB=None,\n",
    "                                connectionstyle=\"arc,rad=0\"\n",
    "                                ))\n",
    "ax.annotate(\"{:04.2f}%\".format(percent_PV), \n",
    "                    xy=(rectPV[0].get_width() / 2, 1),\n",
    "                    xytext=(-10, 60), color='black',\n",
    "                    textcoords=\"offset points\",\n",
    "                    ha='right', va='center',  weight='bold', arrowprops=dict(arrowstyle=\"-\", color=\"black\",\n",
    "                                shrinkA=10, shrinkB=0,\n",
    "                                patchA=None, patchB=None,\n",
    "                                connectionstyle=\"arc,rad=0\"\n",
    "                                ))\n",
    "ax.set_xticks(np.linspace(0, 100, 11, endpoint=True))\n",
    "ax.set_xlim([0, 100])\n",
    "ax.get_xaxis().set_tick_params(labelsize=12)\n",
    "ax.set_ylim([0.75, 1.25])\n",
    "ax.get_yaxis().set_visible(False)\n",
    "# ax.set_title(\"% of neurons\", fontdict={\"size\":12})\n",
    "tight_layout()\n",
    "savefig(join(OUTPUT_FOLDER,\"Percentage_inh_exc.png\"), dpi=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Percentage of incorrect regions before correction\n",
    "len(names) / len(uniques) * 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Percentage of incorrect regions after correction\n",
    "(len(names) - len(np.where(corrected)[0])) / len(uniques) *100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Percentage of volume of the brain occupied by the region incorrect after correction\n",
    "np.sum(volumes[np.where(~corrected)]) / rv[name2allname[\"Basic cell groups and regions\"]] * 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Percentage of neurons of the brain in the region incorrect after correction\n",
    "np.sum([num_neurons[name] for name in names[np.where(~corrected)]]) / np.sum(neu_dens) * 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter regions after correction\n",
    "filter_ = np.ones(names.shape, dtype=bool)\n",
    "# filter_ = np.zeros(names.shape, dtype=bool)\n",
    "# for iname, name in enumerate(names):\n",
    "#     if \"Isocortex\" in name:\n",
    "#         filter_[iname] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig = figure(figsize=(20,25))\n",
    "width = 0.5\n",
    "sum_ = num_SSTs + num_PVs + num_VIPs\n",
    "sum_ = sum_[filter_]\n",
    "x = np.arange(len(sum_))\n",
    "bar1 = barh(x-width/2.0, num_VIPs[filter_]/sum_, height=width, left=(num_SSTs[filter_] + num_PVs[filter_])/sum_, color = \"orange\", xerr=std_sums[:,filter_]/sum_)\n",
    "bar2 = barh(x-width/2.0, num_PVs[filter_]/sum_, height=width, left=num_SSTs[filter_]/sum_, color = \"green\")\n",
    "bar3 = barh(x-width/2.0, num_SSTs[filter_]/sum_, height=width, color = \"darkblue\")\n",
    "bar4 = barh(x+width/2.0, num_GADs[filter_]/sum_, height=width, color = \"red\", xerr=std_GADs[:, filter_]/sum_)\n",
    "# fig.axes[0].yaxis.set_major_locator(FixedLocator((x.astype(float)+0.5)[np.where(corrected[filter_])]))\n",
    "# fig.axes[0].yaxis.set_tick_params(which='major', labelcolor='b')\n",
    "\n",
    "legend((bar1[0], bar2[0], bar3[0], bar4[0]), ('VIP', 'PV', 'SST', 'GAD'))\n",
    "yticks((x.astype(float)+0.5), loc_names[filter_])\n",
    "ylim([0, len(x)-1])\n",
    "tight_layout()\n",
    "savefig(join(OUTPUT_FOLDER, 'Inconsistencies_GAD_sum.png'), dpi=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Excitatory density\n",
    "exc_dens = neu_dens - dens_GAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ratio of excitatory/inhibitory neurons in the Isocortex\n",
    "ids_reg = np.concatenate((children[name2allname[\"Primary somatosensory area\"]], [region_dictionary_to_id[\"Primary somatosensory area\"]]))\n",
    "filter_ = np.in1d(annotation, ids_reg).reshape(annotation.shape)\n",
    "np.sum(dens_GAD[filter_])/np.sum(neu_dens[filter_])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
