{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nrrd\n",
    "from os.path import *\n",
    "from pylab import *\n",
    "from JSONread import *\n",
    "from density_function import *\n",
    "import json\n",
    "\n",
    "DATA_FOLDER = \"data/\"\n",
    "OUTPUT_FOLDER = \"output/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation, h = nrrd.read(join(DATA_FOLDER, \"annotations.nrrd\"))\n",
    "neu_dens, h = nrrd.read(join(DATA_FOLDER, \"neu_density.nrrd\"))\n",
    "num_neurons = json.loads(open(join(DATA_FOLDER, \"neuron_counts.json\"), \"r\").read())\n",
    "jsontextfile = open(join(DATA_FOLDER, \"brain_regions.json\"), \"r\")\n",
    "jsoncontent = json.loads(jsontextfile.read())\n",
    "search_children(jsoncontent['msg'][0])\n",
    "rv = json.loads(open(join(DATA_FOLDER, \"volumes_25.json\"), \"r\").read())\n",
    "voxel_volume = 25**3/1.0e9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing densities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniques = find_unique_regions(annotation, \n",
    "                        id_to_region_dictionary_ALLNAME, \n",
    "                        region_dictionary_to_id_ALLNAME,\n",
    "                        region_dictionary_to_id_ALLNAME_parent, \n",
    "                        name2allname)\n",
    "\n",
    "children, order_ = find_children(uniques, id_to_region_dictionary_ALLNAME, is_leaf,\n",
    "                                  region_dictionary_to_id_ALLNAME_parent, \n",
    "                                  region_dictionary_to_id_ALLNAME)\n",
    "uniques = uniques[np.argsort(order_)] # order from leaf to biggest regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### Choose Markers and their files\n",
    "marker_names = [\"GAD\"] # Careful with the order: should work with the literature file\n",
    "volume_files = [\"479_gad1_expr.npy\"]\n",
    "literature_file = join(DATA_FOLDER, \"gaba_papers.xlsx\")\n",
    "sheet_indices = [0, 1]\n",
    "num_first_row = 1\n",
    "column_name = 0\n",
    "columns_mean = [[1]]\n",
    "num_marker = len(marker_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading results of the fitting pipeline\n",
    "alphas = np.zeros((num_marker, 3))\n",
    "std_fitt = np.zeros((num_marker, 3))\n",
    "for i_marker, name in enumerate(marker_names):\n",
    "    jsoncontent = json.loads(open(join(OUTPUT_FOLDER, \"fitting_\" + name + \".json\"), \"r\").read())\n",
    "    for i_key, key in enumerate([\"Cerebellum\", \"Isocortex\", \"Rest\"]):\n",
    "        if key in jsoncontent[\"alphas\"].keys():\n",
    "            alphas[i_marker][i_key] = jsoncontent[\"alphas\"][key]\n",
    "            std_fitt[i_marker][i_key] = jsoncontent[\"std\"][key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "markers_intensity = np.zeros((num_marker,) + annotation.shape)\n",
    "for i_marker, filename in enumerate(volume_files):\n",
    "    markers_intensity[i_marker] = np.load(join(DATA_FOLDER, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "names, mean_literature, down_std_literature, up_std_literature, convert = read_densities_sheet(\n",
    "                    literature_file, region_keys, columns_mean,\n",
    "                    sheet_indices, num_first_row, column_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In case of multiple literature values for a region, \n",
    "# taking the mean of the means and min and max for stds\n",
    "unames, ucounts = np.unique(names, return_counts=True)\n",
    "mean_literature = mean_literature[0]\n",
    "down_std_literature = down_std_literature[0]\n",
    "up_std_literature = up_std_literature[0]\n",
    "for name in unames[np.where(ucounts>1)]:\n",
    "    filter_ = names==name\n",
    "    names = np.delete(names, np.where(filter_))\n",
    "    names = np.append(names, name)\n",
    "    \n",
    "    GAD = np.mean(mean_literature[filter_])\n",
    "    mean_literature = np.delete(mean_literature, np.where(filter_))\n",
    "    mean_literature = np.append(mean_literature, GAD)\n",
    "    \n",
    "    down_std = np.min(down_std_literature[filter_])\n",
    "    down_std_literature = np.delete(down_std_literature, np.where(filter_))\n",
    "    down_std_literature = np.append(down_std_literature, down_std)\n",
    "    \n",
    "    up_std = np.max(up_std_literature[filter_])\n",
    "    up_std_literature = np.delete(up_std_literature, np.where(filter_))\n",
    "    up_std_literature = np.append(up_std_literature, up_std)\n",
    "    \n",
    "mean_literature = mean_literature.reshape((num_marker,)+mean_literature.shape)\n",
    "up_std_literature = up_std_literature.reshape((num_marker,)+up_std_literature.shape)\n",
    "down_std_literature = down_std_literature.reshape((num_marker,)+down_std_literature.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[\u001b[92m====================================================================================================\u001b[0m] 100%\n"
     ]
    }
   ],
   "source": [
    "# Principal loop, should take a while\n",
    "dens_markers, std_markers = place_cells(annotation, uniques, children, \n",
    "                                        rv, neu_dens, num_neurons,\n",
    "                                        markers_intensity, alphas, std_fitt, \n",
    "                                        names, mean_literature, down_std_literature, up_std_literature,\n",
    "                                        id_to_region_dictionary_ALLNAME, is_leaf, id_to_region_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dens_markers is in cell/voxel, convert in cell per mm3\n",
    "dens_markers /= voxel_volume\n",
    "for i_marker, name in enumerate(marker_names):\n",
    "    nrrd.write(join(OUTPUT_FOLDER, \"densities_\" + name + \".nrrd\"), dens_markers[i_marker], header=h)\n",
    "    with open(join(OUTPUT_FOLDER, \"std_\" + name + \".json\"), 'w') as fp:\n",
    "        json.dump(std_markers[i_marker], fp, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
